# -*- coding: utf-8 -*-
"""enron_email_clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Qy--8QZQs78kkYKioYr_AIm8kcn8nO49
"""

# !unzip /content/drive/MyDrive/emails.csv.zip

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import NMF
import re

"""# Preparing the data"""

# loading the data
df = pd.read_csv('/content/drive/MyDrive/sample_emails (1).csv')
df.drop(columns=['Unnamed: 0', 'file'], inplace=True)

def prepare(in_text: str):
    txt = in_text.split('\n')
    txt = [i.strip() for i in txt if len(i) > 0]

    out = []
    index = None

    for i in txt:
        if (i[:3] == 'To:') and (i[-4:] == '.com'):
            out.append(i)
        elif (i[:5] == 'From:') and (i[-9:] == 'enron.com'):
            out.append(i)
        elif i[:11] == 'X-FileName:':
            index = txt.index(i)
    if index:
        txt = txt[index + 1:]
        txt = '\n'.join(txt)
        txt = re.sub(r'\s\s', ' ', txt)
        out.append(txt)
    return out

data = pd.DataFrame()
data['raw'] = df['message'].apply(lambda x: prepare(x))
data['len'] = data['raw'].apply(lambda x: len(x))
data['targets'] = data['raw'].apply(lambda x: [i for i in x if i[:3] == 'To:'])
data['sources'] = data['raw'].apply(lambda x: [i for i in x if i[:5] == 'From:'])
data['message'] = data['raw'].apply(lambda x: [i for i in x if ((i[:3] != 'To:') & (i[:5] != 'From:'))])
data['len_tar'] = data['targets'].apply(lambda x: len(x))
data['len_src'] = data['sources'].apply(lambda x: len(x))
data = data[data['len_tar'] != 0]
data = data[data['len_src'] != 0]
data['message'] = data['message'].apply(lambda x: str(x[0]) if len(x) > 0 else x)
data.reset_index(inplace=True)
data.drop(columns=['index'], inplace=True)
data = data.explode('targets')
data = data.explode('sources')

# data['targets'] = data['targets'].apply(lambda x: x[4:])
# data['sources'] = data['sources'].apply(lambda x: x[6:])

data.reset_index(inplace=True)

data.drop(columns=['index', 'len_tar', 'len_src', 'len', 'raw'], inplace=True)

"""# Text clustering

## 1. KMeans
"""

vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(data['message'])

# clustering the data
model_kmeans = KMeans(n_clusters=5)
model_kmeans.fit(X)

order_centroids = model_kmeans.cluster_centers_.argsort()[:, ::-1]
for i in range(5):
    print(f"Cluster {i + 1}:")
    for idx in order_centroids[i, :10]:
        print(f"{vectorizer.get_feature_names_out()[idx]}", end=' ')
    print()

for idx, topic in enumerate(order_centroids):
    print(f"{idx}: {[vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-5:]]}")

vectorizer.get_feature_names_out().shape



"""## 2. NMF"""

vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(data['message'])

model_nmf = NMF(n_components=5, random_state=0)
model_nmf.fit(X)

topics = {}
for idx, topic in enumerate(model_nmf.components_):
    topics[idx] = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-5:]]

for topic in topics:
    print(f"topic {topic}: {topics[topic]}", end='\n\n')

data

data.to_csv('./enron_sample.csv')

