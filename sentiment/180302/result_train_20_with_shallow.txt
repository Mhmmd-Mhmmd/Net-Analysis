/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
 [ 801/8000 10:41 < 1:36:16, 1.25 it/s, Epoch 2/20]
Epoch	Training Loss	Validation Loss	Accuracy
1	0.552300	0.517359	0.777500
 [45/50 00:24 < 00:02, 1.82 it/s]
 [8000/8000 1:53:11, Epoch 20/20]
Epoch	Training Loss	Validation Loss	Accuracy
1	0.552300	0.517359	0.777500
2	0.250400	0.325317	0.906250
3	0.045900	0.420954	0.900000
4	0.000900	0.581655	0.903750
5	0.116400	0.601060	0.885000
6	0.003900	0.645634	0.897500
7	0.000600	0.741172	0.911250
8	0.000100	0.771958	0.905000
9	0.000100	0.998605	0.891250
10	0.000100	0.716882	0.903750
11	0.040400	0.768775	0.912500
12	0.000100	0.766606	0.911250
13	0.000000	0.743979	0.921250
14	0.000100	0.665273	0.916250
15	0.000100	0.712741	0.911250
16	0.001500	0.775298	0.918750
17	0.001500	0.788329	0.918750
18	0.000000	0.767422	0.920000
19	0.000000	0.791401	0.920000
20	0.000000	0.796428	0.920000
TrainOutput(global_step=8000, training_loss=0.09055505945578443, metrics={'train_runtime': 6795.543, 'train_samples_per_second': 9.418, 'train_steps_per_second': 1.177, 'total_flos': 1.6839258734592e+16, 'train_loss': 0.09055505945578443, 'epoch': 20.0})
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
 [8000/8000 1:52:04, Epoch 20/20]
Epoch	Training Loss	Validation Loss	Accuracy
1	No log	0.279146	0.896250
2	0.468600	0.312528	0.922500
3	0.200100	0.432730	0.916250
4	0.098700	0.611436	0.898750
5	0.050700	0.547060	0.917500
6	0.050700	0.676081	0.912500
7	0.025500	0.574387	0.925000
8	0.037900	0.675262	0.910000
9	0.022800	0.537144	0.925000
10	0.016100	0.768329	0.917500
11	0.016100	0.904490	0.902500
12	0.017600	0.653365	0.925000
13	0.021800	0.785956	0.910000
14	0.010100	0.714299	0.921250
15	0.005400	0.795409	0.913750
16	0.005400	0.807825	0.913750
17	0.008800	0.839540	0.913750
18	0.004000	0.837033	0.916250
19	0.003900	0.842544	0.916250
20	0.005500	0.845906	0.916250
TrainOutput(global_step=8000, training_loss=0.06233530829846859, metrics={'train_runtime': 6729.4222, 'train_samples_per_second': 9.51, 'train_steps_per_second': 1.189, 'total_flos': 1.6839258734592e+16, 'train_loss': 0.06233530829846859, 'epoch': 20.0})
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------