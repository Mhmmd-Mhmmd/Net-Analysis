# -*- coding: utf-8 -*-
"""00_pytorch_geometry_edge_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jqm58XQmpKk4LfZ4UiRREOE4zYQiCZVl
"""

try:
    import torch_geometric
except ModuleNotFoundError:
    !pip install torch_geometric

import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
from torch import nn
from torch_geometric.data import Data
from torch_geometric import nn as geom_nn
from sklearn.model_selection import train_test_split

df = pd.read_csv('/content/enron_sample.csv')
df.drop(columns='Unnamed: 0', inplace=True)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF
import spacy

nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])

def preprocess_text(in_text):
    doc = nlp(in_text)

    tokens = [token.lemma_.lower().strip() for token in doc if not token.is_stop and not token.is_punct]
    text = ' '.join(tokens)

    return text

df['text'] = df['message'].apply(lambda x: preprocess_text(x))

n_topics = 5

text_vectorizer = TfidfVectorizer(stop_words='english')
X = text_vectorizer.fit_transform(df['text'])

model_nmf = NMF(n_components=n_topics, random_state=0)
model_nmf.fit(X)

topics = {}
for idx, topic in enumerate(model_nmf.components_):
    topics[idx] = [text_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-5:]]
topics

result = model_nmf.transform(X)
df['label'] = result.argmax(axis=1)

df['targets'] = df['targets'].apply(lambda x: x[3:] if x[:3] == 'To:' else x )
df['sources'] = df['sources'].apply(lambda x: x[5:] if x[:5] == 'From:' else x )

df

data = df[['targets', 'sources', 'label']]

data

node_dict = {}
name_dict = {}
for i, node in enumerate(np.unique(data[['targets', 'sources']])):
    node_dict[i] = node
    name_dict[node] = i

data['targets'] = data['targets'].apply(lambda x: name_dict[x])
data['sources'] = data['sources'].apply(lambda x: name_dict[x])

data

edge_list_array = np.array(data)
edge_list_array.shape

edge_list= torch.from_numpy(edge_list_array)

source = edge_list[:, 1]
target = edge_list[:, 0]
label  = edge_list[:, 2]

edge_index = torch.stack([source, target], dim=0)

edge_index

torch.stack([source, target], dim=0).shape

data = Data(edge_index=edge_index, y=label)

data

print(data.edge_index[0])
data.edge_index

deg = torch_geometric.utils.degree(data.edge_index[0])
deg = deg / deg.max()

x = deg.view(-1, 1)

x.shape

data.edge_index.shape

x.shape

data.x = x

data

class GNN(nn.Module):
    def __init__(self, num_node_features, num_classes):
        super(GNN, self).__init__()

        # defining conv layers
        self.conv1 = geom_nn.GCNConv(in_channels=num_node_features, out_channels=16)
        self.conv2 = geom_nn.GCNConv(in_channels=16, out_channels=32)

        # defining linear layers
        self.fc1 = nn.Linear(in_features=32, out_features=64)
        self.fc2 = nn.Linear(in_features=64, out_features=num_classes)

    def forward(self, data):
        x = data.x
        edge_index = data.edge_index

        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        x = F.relu(x)

        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)

        return x

epochs = 10
batch_size = 32
lr = 0.01

data.y.unique() # num classes

model = GNN(data.num_node_features, len(data.y.unique()))

model

device = 'cuda' if torch.cuda.is_available() else 'cpu'
model.to(device)

loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)

type(data)

dataset = torch_geometric.data.InMemoryDataset(data)
train_dataset, test_dataset = dataset.split([0.8], shuffle=True)

!pip install torch_geometric

" SGC model using torch geometric with Cora"
" Node2Vec model using torch geometric with Cora"
import torch
from torch_geometric.datasets import Planetoid # The citation network datasets “Cora”, “CiteSeer” and “PubMed”
from torch_geometric.nn import SGConv # Import Node2Vec Model
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import torch.nn.functional as F

" **************** IMPORT DATA ********************"
path = "C:/Users/Younes/Desktop"  # Directory to download dataset
dataset = Planetoid(path, "Cora") # Download the dataset
data = dataset[0] # Tensor representation of the Cora-Planetoid data
print('Cora: ', data)

device = 'cuda' if torch.cuda.is_available() else 'cpu'

" **************** CONSTRUCT THE MODEL  ********************"
SGC_model = SGConv(in_channels= data.num_features, # Number of features
                   out_channels= dataset.num_classes, # Dimension of embedding
                   K = 1, cached =True)

" **************** GET EMBEDDING  ********************"
print(" Shape of the original data: ", data.x.shape)
print(" Shape of the embedding data: ", SGC_model(data.x,data.edge_index).shape)

" **************** CONSTRUCT THE MODEL FOR CLASSIFICATION  ********************"
class SGCNet(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = SGConv(in_channels= data.num_features, # Number of features
                   out_channels= dataset.num_classes, # Dimension of embedding
                   K = 1, cached =True)

    def forward(self):
        x = self.conv1(data.x,  data.edge_index) #Applying convolution to data

        # computation of log softmax
        return F.log_softmax(x, dim=1)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
SGC_model, data = SGCNet().to(device), data.to(device)
optimizer = torch.optim.Adam(SGC_model.parameters(), lr=0.2, weight_decay=0.005)

# What are the learning parameters:
for i, parameter in SGC_model.named_parameters():
    print(" Parameter {}".format(i))
    print("Shape: ",parameter.shape)

" **************** TRAIN FUNCTION ********************"
def train():
    SGC_model.train() # Set the model.training to be True
    optimizer.zero_grad() # Reset the gradient
    predicted_y = SGC_model() # predicted y in log softmax prob
    true_y = data.y # True labels
    losses = F.nll_loss(predicted_y[data.train_mask], true_y[data.train_mask])
    losses.backward()
    optimizer.step() # Update the parameters such that is minimized the losses

" **************** TEST FUNCTION ********************"
def test():
    SGC_model.eval() # Set the model.training to be False
    logits = SGC_model() # Log prob of all data
    accs = []
    for _, mask in data('train_mask', 'val_mask', 'test_mask'):
        pred = logits[mask].max(1)[1] #Transforming log prob to actual labels
        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()
        accs.append(acc)
    return accs

" **************** PUTTING IT ALL TOGETHER ********************"
best_val_acc = test_acc = 0
for epoch in range(1, 101):
    train()
    train_acc, val_acc, tmp_test_acc = test()
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        test_acc = tmp_test_acc
    log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'
    print(log.format(epoch, train_acc, best_val_acc, test_acc))

